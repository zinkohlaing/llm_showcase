{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyCU\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Gemini\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(description):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset based on the user's description.\n",
    "    \"\"\"\n",
    "    # Create a prompt for the LLM to generate the dataset\n",
    "    prompt = f\"\"\"\n",
    "    Based on this description: \"{description}\"\n",
    "    \n",
    "    Generate a synthetic dataset in CSV format. The dataset should be realistic and diverse.\n",
    "    Include appropriate column headers and at least 50 rows of data.\n",
    "    \n",
    "    Return ONLY the CSV content, nothing else. The first line should be the header row.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get response from OpenAI\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # Get the CSV content\n",
    "    csv_content = response.choices[0].message.content\n",
    "    \n",
    "    # Save to a temporary file\n",
    "    filename = \"generated_dataset.csv\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(csv_content)\n",
    "    \n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=generate_dataset,\n",
    "    inputs=gr.Textbox(\n",
    "        label=\"Describe the dataset you want to generate\",\n",
    "        placeholder=\"e.g., I want to build a synthetic dataset of an Ecommerce store with customer purchases\"\n",
    "    ),\n",
    "    outputs=gr.File(label=\"Download your dataset\"),\n",
    "    title=\"Synthetic Dataset Generator\",\n",
    "    description=\"Describe the type of dataset you want to generate, and we'll create it for you!\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Gradle UI with models selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client  # For local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama client for local model\n",
    "ollama_client = Client(host='http://localhost:11434')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama pull mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_cost(model_choice, input_tokens, output_tokens):\n",
    "    \"\"\"\n",
    "    Estimate the cost based on the model and token counts.\n",
    "    Prices are per 1K tokens (as of April 2024).\n",
    "    \"\"\"\n",
    "    # Updated prices\n",
    "    prices = {\n",
    "        \"OpenAI (GPT-3.5-turbo)\": {\n",
    "            \"input\": 0.0010,\n",
    "            \"output\": 0.0020\n",
    "        },\n",
    "        \"Anthropic (Claude 3 Haiku)\": {\n",
    "            \"input\": 0.00025,\n",
    "            \"output\": 0.00125\n",
    "        },\n",
    "        \"Google (Gemini 2.0 Flash Lite)\": {\n",
    "            \"input\": 0.0001,  # Updated for Gemini 2.0 Flash Lite\n",
    "            \"output\": 0.0002\n",
    "        },\n",
    "        \"Local (Mistral 7B)\": {\n",
    "            \"input\": 0.0,\n",
    "            \"output\": 0.0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    model_prices = prices[model_choice]\n",
    "    input_cost = (input_tokens / 1000) * model_prices[\"input\"]\n",
    "    output_cost = (output_tokens / 1000) * model_prices[\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return f\"Estimated cost: ${total_cost:.4f} (Input: ${input_cost:.4f}, Output: ${output_cost:.4f})\"\n",
    "\n",
    "def generate_dataset(description, model_choice, num_rows):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset based on the user's description and selected model.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Based on this description: \"{description}\"\n",
    "    \n",
    "    Generate a synthetic dataset in CSV format. The dataset should be realistic and diverse.\n",
    "    Include appropriate column headers and exactly {num_rows} rows of data.\n",
    "    \n",
    "    Return ONLY the CSV content, nothing else. The first line should be the header row.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Estimate input tokens (rough approximation)\n",
    "        input_tokens = len(prompt.split()) * 1.3  # Approximate tokens per word\n",
    "        \n",
    "        if model_choice == \"OpenAI (GPT-3.5-turbo)\":\n",
    "            try:\n",
    "                response = openai.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                csv_content = response.choices[0].message.content\n",
    "                output_tokens = response.usage.completion_tokens\n",
    "            except Exception as e:\n",
    "                return [f\"OpenAI Error: {str(e)}\", \"Cost estimation unavailable\", \"<p>Preview unavailable</p>\"]\n",
    "            \n",
    "        elif model_choice == \"Anthropic (Claude 3 Haiku)\":\n",
    "            try:\n",
    "                response = claude.messages.create(\n",
    "                    model=\"claude-3-haiku-20240307\",\n",
    "                    max_tokens=1000,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "                csv_content = response.content[0].text\n",
    "                output_tokens = len(csv_content.split()) * 1.3  # Approximate for Claude\n",
    "            except Exception as e:\n",
    "                return [f\"Anthropic Error: {str(e)}\", \"Cost estimation unavailable\", \"<p>Preview unavailable</p>\"]\n",
    "            \n",
    "        elif model_choice == \"Google (Gemini 2.0 Flash Lite)\":\n",
    "            try:\n",
    "                model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "                response = model.generate_content(prompt)\n",
    "                csv_content = response.text\n",
    "                output_tokens = len(csv_content.split()) * 1.3  # Approximate for Gemini\n",
    "            except Exception as e:\n",
    "                return [f\"Google Gemini Error: {str(e)}\", \"Cost estimation unavailable\", \"<p>Preview unavailable</p>\"]\n",
    "            \n",
    "        elif model_choice == \"Local (Mistral 7B)\":\n",
    "            try:\n",
    "                response = ollama_client.generate(\n",
    "                    model='mistral',\n",
    "                    prompt=prompt,\n",
    "                    stream=False\n",
    "                )\n",
    "                csv_content = response['response']\n",
    "                output_tokens = len(csv_content.split()) * 1.3  # Approximate for Mistral\n",
    "            except Exception as e:\n",
    "                return [f\"Local Model Error: {str(e)}\", \"Cost estimation unavailable\", \"<p>Preview unavailable</p>\"]\n",
    "        \n",
    "        # Save to a temporary file\n",
    "        filename = \"generated_dataset.csv\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(csv_content)\n",
    "        \n",
    "        # Calculate cost estimate\n",
    "        cost_estimate = estimate_cost(model_choice, input_tokens, output_tokens)\n",
    "        \n",
    "        # Convert CSV to DataFrame for preview\n",
    "        import pandas as pd\n",
    "        from io import StringIO\n",
    "        try:\n",
    "            df = pd.read_csv(StringIO(csv_content))\n",
    "            # Verify number of rows matches request\n",
    "            if len(df) != num_rows:\n",
    "                preview_html = f\"<p>Warning: Generated {len(df)} rows instead of requested {num_rows} rows.</p>\" + df.to_html(index=False, classes='table table-striped')\n",
    "            else:\n",
    "                preview_html = df.to_html(index=False, classes='table table-striped')\n",
    "        except Exception as e:\n",
    "            preview_html = f\"<p>Error creating preview: {str(e)}</p>\"\n",
    "        \n",
    "        return [filename, cost_estimate, preview_html]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return [f\"General Error: {str(e)}\", \"Cost estimation unavailable\", \"<p>Preview unavailable</p>\"]\n",
    "\n",
    "# Create the Gradio interface with model selection, cost estimation, preview, and row control\n",
    "with gr.Blocks(theme=gr.themes.Soft(\n",
    "    primary_hue=\"blue\",\n",
    "    secondary_hue=\"gray\",\n",
    "    neutral_hue=\"slate\",\n",
    "    font=[\"Inter\", \"sans-serif\"]\n",
    ")) as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    <style>\n",
    "    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap');\n",
    "    \n",
    "    body {\n",
    "        font-family: 'Inter', sans-serif !important;\n",
    "    }\n",
    "    \n",
    "    .table {\n",
    "        width: 100%;\n",
    "        border-collapse: collapse;\n",
    "        margin: 1em 0;\n",
    "        font-family: 'Inter', sans-serif !important;\n",
    "    }\n",
    "    .table th, .table td {\n",
    "        padding: 8px;\n",
    "        border: 1px solid #e2e8f0;\n",
    "        text-align: left;\n",
    "        font-family: 'Inter', sans-serif !important;\n",
    "    }\n",
    "    .table th {\n",
    "        background-color: #f8fafc;\n",
    "        font-weight: 600;\n",
    "    }\n",
    "    .table-striped tr:nth-child(even) {\n",
    "        background-color: #f8fafc;\n",
    "    }\n",
    "    \n",
    "    /* Custom styles for Gradio components */\n",
    "    .gradio-container {\n",
    "        font-family: 'Inter', sans-serif !important;\n",
    "    }\n",
    "    \n",
    "    .gradio-button {\n",
    "        font-family: 'Inter', sans-serif !important;\n",
    "        font-weight: 500;\n",
    "    }\n",
    "    \n",
    "    .gradio-input {\n",
    "        font-family: 'Inter', sans-serif !important;\n",
    "    }\n",
    "    \n",
    "    .gradio-output {\n",
    "        font-family: 'Inter', sans-serif !important;\n",
    "    }\n",
    "    \n",
    "    /* Custom scrollbar */\n",
    "    ::-webkit-scrollbar {\n",
    "        width: 8px;\n",
    "        height: 8px;\n",
    "    }\n",
    "    \n",
    "    ::-webkit-scrollbar-track {\n",
    "        background: #f1f5f9;\n",
    "    }\n",
    "    \n",
    "    ::-webkit-scrollbar-thumb {\n",
    "        background: #cbd5e1;\n",
    "        border-radius: 4px;\n",
    "    }\n",
    "    \n",
    "    ::-webkit-scrollbar-thumb:hover {\n",
    "        background: #94a3b8;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\")\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style='text-align: center; margin-bottom: 20px;'>\n",
    "        <h1 style='font-family: \"Inter\", sans-serif; font-weight: 600; color: #1e293b;'>Synthetic Dataset Generator</h1>\n",
    "        <p style='font-family: \"Inter\", sans-serif; color: #64748b;'>Generate synthetic datasets using different LLMs. Choose your preferred model and describe the dataset you want to create!</p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            description = gr.Textbox(\n",
    "                label=\"Describe the dataset you want to generate\",\n",
    "                placeholder=\"e.g., I want to build a synthetic dataset of an Ecommerce store with customer purchases\",\n",
    "                lines=3\n",
    "            )\n",
    "            model_choice = gr.Dropdown(\n",
    "                choices=[\n",
    "                    \"OpenAI (GPT-3.5-turbo)\",\n",
    "                    \"Anthropic (Claude 3 Haiku)\",\n",
    "                    \"Google (Gemini 2.0 Flash Lite)\",\n",
    "                    \"Local (Mistral 7B)\"\n",
    "                ],\n",
    "                value=\"OpenAI (GPT-3.5-turbo)\",\n",
    "                label=\"Select Model\"\n",
    "            )\n",
    "            num_rows = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=100,\n",
    "                value=10,\n",
    "                step=1,\n",
    "                label=\"Number of Rows\",\n",
    "                info=\"Choose how many rows of data to generate\"\n",
    "            )\n",
    "            generate_btn = gr.Button(\"Generate Dataset\", variant=\"primary\")\n",
    "            \n",
    "        with gr.Column():\n",
    "            cost_estimate = gr.Textbox(\n",
    "                label=\"Cost Estimation\",\n",
    "                interactive=False,\n",
    "                elem_classes=[\"cost-estimate\"]\n",
    "            )\n",
    "            preview = gr.HTML(label=\"Data Preview\")\n",
    "            output_file = gr.File(label=\"Download your dataset\")\n",
    "    \n",
    "    generate_btn.click(\n",
    "        fn=generate_dataset,\n",
    "        inputs=[description, model_choice, num_rows],\n",
    "        outputs=[output_file, cost_estimate, preview]\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
